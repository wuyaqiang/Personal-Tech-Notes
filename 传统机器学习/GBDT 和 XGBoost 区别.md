1. 优化方法：GBDT 利用梯度下降法进行优化，而 XGBoost 利用牛顿法进行优化，因此，前者只计算一阶梯度，而后者计算二阶梯度，能加快收敛速度。



2. 正则项：GBDT 没有正则项，XGBoost 中加入了正则项来防止过拟合。



3. XGBoost 增加了处理缺失值的方案。



4. XGBoost 借鉴了随机森林的做法，支持特征抽样，防止过拟合，并减少计算量。



5. XGBoost 支持特征粒度上的并行(不是树粒度上并行，因为整体还是boosting算法，需要串行生成树)，在确定最佳分割点之前，提前对特征进行排序。

