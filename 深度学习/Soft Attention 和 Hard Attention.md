NLP 中使用的注意力机制，基本都属于 Soft Attention 。

以机器翻译为例，Soft Attention 将输入句子中的每个单词都计算出一个概率，得到一个概率分布；而 Hard Attention 的想法是直接从句子中找到某个特定的单词，然后把目标句子单词和这个单词对齐，输入句子中其他单词的对齐概率全为0，这种单词一一对齐难度较大，如果对不齐对后续处理产生很大负面影响。

因此，Hard Attention 可能在图像中有一定用处，但是在NLP中使用比较少。

Soft Attention 是参数化的，可直接求导，可以在模型中参与训练，根据梯度进行反向传播；Hard Attention 则不行。
