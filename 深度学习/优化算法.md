## 一、梯度下降
计算目标函数的梯度时，可以根据使用的数据量来区分梯度下降算法的三个变种：

1、Batch gradient descent  
使用整个训练数据集来计算目标函数的梯度，然后进行一次更新  
收敛速度较慢，无法在线更新，且当整个数据集无法装入内存时，无法计算  

2、Stochastic gradient descent  
每次更新仅仅使用一个训练样例（ x(i) , y(i) ）来计算梯度  
Batch gradient descent在每次更新时，会重复对相同的样例计算梯度，造成计算冗余，SGD就避免了这种冗余性，所以收敛速度更快，且能够在线学习  
SGD的频繁更新，方差较大，会导致目标函数出现波动  
每个epoch初始时刻，都要打散整个训练集

3、Mini-batch gradient descent  
每次更新一个mini-batch（包含n个训练样例）  
减小了参数更新的方差，使收敛更加稳定  
可以利用高度优化的矩阵运算，以及并行计算框架，效率很高  

随机梯度下降的缺点：  
SGD 放弃了梯度的准确性，仅采用一部分样本来估计当前的梯度；因此 SGD 对梯度的估计常常出现偏差，造成目标函数收敛不稳定，甚至不收敛的情况。  
可能陷入局部极值点，可能遇到“峡谷”和“鞍点”两种情况。  
