首先，防止过拟合的方法取决于你使用的训练算法，同时还受数据集的影响。

深度学习中：

最常用的肯定是训练过程中使用的dropout方法

L2正则项很常用，L1在正则化时较少使用

一般来讲，dropout + L2 就够用了

Dropout:

A 0.5 to 0.75 dropout rate between fully connected layers is going to work quite well and a 0.1–0.3 dropout rate would typically do better for the input layer.

But as a rule of thumb: the more data you have, the less dropout you actually need.

关于模型复杂度有一条经验法则：your total number of weights in your hidden layers should be of maximum 1/2 of your training samples. You can trick this rule with dropout a bit but that’s about it.


另外还有一些方法：
* 交叉验证
* Early Stopping 提早停止
* Data Augmentation(数据增强)：图像中，平移、旋转、缩放等，NLP中，利用机器翻译生成新数据
* 利用生成对抗网络生成新数据
* 降低模型复杂度：神经网络中，减少网络层、神经元个数，决策树中，降低树的深度、剪枝等


在SVM算法中：

The major trick for avoiding overfitting in SVMs is to carefully tune the regularization parameter C, along with the kernel parameters (in case you are running a kernel (non linear) SVM). The tuning of the parameters is generally done using cross validation or the validation set.

在决策树算法中：

可以采用预剪枝、后剪枝等

以下观点来自Quora：

The training convergence here depends on the type of classes in the dataset that you aim to discriminate. Like for CNNs, in case the classes can be separated easily in the image space (like objects), then the accuracy will be high and convergence will be slow, since the network will see enough opportunities to separate the classes out. On the other hand, for some classes which are abstract in nature (such as face attributes, fashion garment attributes, fine grained classes), their discrimination in the image space is not easy to obtain, and thus you might see faster convergence, perhaps reasonable accuracy, but lesser generalization potential. This is exactly not overfitting, but is a kind of overfitting. Here, the network is learning only a few (or even wrong) aspects of discrimination, because of the limitations of the underlying operations and architecture, and not because the model is complex or there are too many parameters.
