### 一、激活函数的作用

向网络中加入非线性因素，加强网络的表示能力，解决线性模型无法解决的问题。

神经网络的万能近似定理认为只要神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。

如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合，无论网络有多少层，其整体也是线性的，就无法满足上述定理。

但仅部分层是纯线性是可以接受的，这有助于减少网络中的参数。

### 二、常用激活函数及其对比

非线性： Relu，sigmoid，tanh   
线性：softmax(常作为模型输出层，表示了具有k个可能值的离散型随机变量的概率分布)

Relu 的扩展：   
g(z; a) = max(0, z) + a*min(0, z)   
绝对值整流、Leaky Relu、parametric Relu 等都是上式中 a 为不同值的 Relu 的扩展。   

Relu 相比 sigmoid 的优点：   
* 避免梯度消失   
* 减缓过拟合
* 加速计算

知乎详细回答：  
[https://www.zhihu.com/question/52020211/answer/152378276](https://www.zhihu.com/question/52020211/answer/152378276)  
[https://www.zhihu.com/question/29021768/answer/43488153](https://www.zhihu.com/question/29021768/answer/43488153)

